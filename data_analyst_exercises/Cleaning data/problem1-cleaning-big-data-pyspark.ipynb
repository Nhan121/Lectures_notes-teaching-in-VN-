{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nt0 = time.time()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Installation `pyspark`"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext, SparkSession\nprint('Installation takes %s seconds'%(time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Build-in a spark-session"},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession.builder \\\n                    .master(\"local\") \\\n                    .appName(\"Word Count\") \\\n                    .config(\"spark.some.config.option\", \"some-value\") \\\n                    .getOrCreate()\nspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = spark.sparkContext\nsqlContext = SQLContext(sc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loading `csv.data` to `spark` & viewing by `pandas`"},{"metadata":{"trusted":true},"cell_type":"code","source":"movement_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'../input/big-data-vers-1/movement.csv')\nmovement_df.toPandas().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visiting_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'../input/big-data-vers-1/visiting.csv')\nvisiting_df.toPandas().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Register the `loaded-dataframe` as table in `SQL`"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframes = [movement_df, visiting_df]\ntable_names = [\"movement\", \"visiting\"]\nfor k in range(2):\n    t0 = time.time()\n    SQLContext.registerDataFrameAsTable(sc, df = dataframes[k], tableName = table_names[k])  #or using visiting_df.createGlobalTempView(table_names[k])\n    print('Attach table %s (%s) takes %s seconds'%(k+1, table_names[k], time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Cleaning data.`table: visiting`\nWhen using data, most people agree that your insights and analysis are only as good as the data you are using. Essentially, garbage data in is garbage analysis out. Data cleaning, also referred to as data cleansing and data scrubbing, is one of the most important steps for your organization if you want to create a culture around quality data decision-making.\n\n\n#### Definition.\nData cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset.\n\nWhen combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled. If data is incorrect, outcomes and algorithms are unreliable, even though they may look correct. There is no one absolute way to prescribe the exact steps in the data cleaning process because the processes will vary from dataset to dataset. But it is crucial to establish a template for your data cleaning process so you know you are doing it the right way every time."},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Fix structural errors\nStructural errors are when you measure or transfer data and notice strange naming conventions, typos, or incorrect capitalization. These inconsistencies can cause mislabeled categories or classes. For example, you may find `“N/A”` and `“Not Applicable”` both appear, but they should be analyzed as the same category.\n\n**Step 1. Viewing the structure at each column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"visiting = visiting_df.toPandas()\nvisiting.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, in this database, all of your `feature` are stored as an `object` or `text type features`, but some of them; such as\n- `utc_timestamp`\n- `local_timestamp`\n- `minimum_dwell`\n- etc\n\nmust be stored as an `numeric`. Look at the following `transformation: object --> numeric`."},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = ['utc_timestamp', 'local_timestamp', 'minimum_dwell']\nfor col in numeric_columns:    \n    if col != 'minimum_dwell':\n        visiting[col] = visiting[col].apply(lambda x: int(x))\n    else:\n\n        visiting[col] = visiting[col].apply(lambda x: float(x))\nvisiting.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 2. Using `countplot` / `barplot`.**\n\nIn this step, check for typos or inconsistent capitalization. This is mostly a concern for categorical features, and you can look at your bar plots to check."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nquery = spark.sql(\"\"\"SELECT COUNT(*) AS count, brands \n                     FROM visiting\n                     GROUP BY brands\n                     ORDER BY count DESC\n                     LIMIT 10\n                 \"\"\")\nquery = query.toPandas()\nsns.barplot(x = 'brands', y = 'count', orient = 'h', data = query)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Remove duplicate or irrelevant observations\n### 2.2.1. Duplicate observations\nRemove unwanted observations from your dataset, including duplicate observations or irrelevant observations. Duplicate observations will happen most often during data collection. When you :\n- combine data sets from multiple places, \n- scrape data, or receive data from clients or multiple departments, there are opportunities to create duplicate data. \n\nDe-duplication is one of the largest areas to be considered in this process.\n\nIn `Python`, `an` observation is corresponding to the `row in DataFrame table`, hence: you can use the function `dataframe.drop_duplicates()` from `pandas` to solve this process."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data-dimension; before drop_duplicates:', visiting.shape)\nvisiting.drop_duplicates(inplace = True)\nprint('Data-dimension; after drop_duplicates:', visiting.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$\\qquad \\Rightarrow$ Hence, there is not any `duplicated row` in this dataset."},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2. Irrelevant observations\n`Irrelevant observations` are when you notice observations that do not fit into the **specific problem** you are trying to analyze. \n\nFor example, \n- If you want to analyze data regarding `millennial customers`, but your dataset includes `older generations`, you might remove those `irrelevant observations`. This can make analysis more efficient and minimize distraction from your primary target—as well as creating a more manageable and more performant dataset.\n\n- If you were building a model for `Single-Family` homes only, you wouldn't want observations for `Apartments` in there.\n\nChecking for `irrelevant observations` before `engineering features` can save you many headaches down the road.\n\n=> Now, come back our dataset: `visiting`\n\n$\\qquad$ **Step 1. Apply the `count plot`**"},{"metadata":{"trusted":true},"cell_type":"code","source":"visiting.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visiting.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So"},{"metadata":{},"cell_type":"markdown","source":"## 2.3."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Cleaning data. `table: movement`"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}